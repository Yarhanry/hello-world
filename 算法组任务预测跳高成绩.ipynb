{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.01852238]]\n"
     ]
    }
   ],
   "source": [
    "def load_data(): #将处理的数据分装成函数\n",
    "    #数据导入\n",
    "    import numpy as np\n",
    "    import json\n",
    "    datafile = 'D:/py学习与代码/算法组 NLP任务三/成绩数据.txt'\n",
    "    data = np.fromfile(datafile,sep=' ')  \n",
    "    #数据形状变换\n",
    "    feature_names = ['跳高','行进跑','跳远','助跑跳高','杠铃','半蹲系数','100','抓举']\n",
    "    feature_num = len(feature_names)\n",
    "    data = data.reshape(14,8)\n",
    "    #归一化处理 \n",
    "    maxi,mini,avgs = data.max(axis=0),data.min(axis=0),data.sum(axis=0)/data.shape[0]\n",
    "    for i in range(feature_num):\n",
    "        #print(maxi[i], mini[i], avgs[i])\n",
    "        data[:,i] = (data[:,i]-avgs[i])/(maxi[i]-mini[i])\n",
    "    return data\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self,num_of_weights):\n",
    "        #s随机产生w的初始值\n",
    "        #为了保持程序每次运行结果的一致性，设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.b = 0\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = np.dot(x,self.w) + self.b\n",
    "        return z\n",
    "    def loss(self, z, y):\n",
    "        error = z-y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)  #对向量取均值操作\n",
    "        return cost\n",
    "    def gradient(self,x,y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w,axis=0)\n",
    "        gradient_w = gradient_w[:,np.newaxis]\n",
    "        gradient_b = (z-y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        return gradient_w,gradient_b\n",
    "    def update(self,gradient_w,gradient_b,eta =0.01): #定义步长eta     \n",
    "        # 梯度更新\n",
    "        # 沿着梯度的反方向移动到下一个点P1 \n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "    def train(self, data, num_epoches, batch_size=1, eta=0.01):\n",
    "        n = len(data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epoches):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)\n",
    "                self.update(gradient_w, gradient_b, eta)\n",
    "                losses.append(loss)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "import numpy as np\n",
    "# 获取数据\n",
    "data1 = load_data()\n",
    "# 创建网络\n",
    "net = Network(7)\n",
    "#启动训练\n",
    "w,b = net.train(data1,num_epoches =65)\n",
    "a=np.array([3.0,9.3,2.05,100,2.8,11.2,50])\n",
    "x=a[np.newaxis,:]\n",
    "score = np.dot(x,w) +b\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
